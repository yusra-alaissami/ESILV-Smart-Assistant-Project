{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZD9FOhrZvhH"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CELLULE 1 : Installation Ollama + Mod√®le\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Installation de l'environnement ESILV Chatbot...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Installer Ollama\n",
        "print(\"\\n[1/5] Installation d'Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"‚úì Ollama install√©\")\n",
        "\n",
        "# 2. Tuer tout processus Ollama existant (au cas o√π)\n",
        "print(\"\\n[2/5] Nettoyage des processus existants...\")\n",
        "!pkill -9 ollama 2>/dev/null || true\n",
        "time.sleep(2)\n",
        "\n",
        "# 3. D√©marrer le serveur Ollama en arri√®re-plan\n",
        "print(\"\\n[3/5] D√©marrage du serveur Ollama...\")\n",
        "subprocess.Popen(['ollama', 'serve'],\n",
        "                 stdout=subprocess.PIPE,\n",
        "                 stderr=subprocess.PIPE)\n",
        "time.sleep(15)  # Attendre que le serveur d√©marre compl√®tement\n",
        "print(\"‚úì Serveur Ollama d√©marr√© sur localhost:11434\")\n",
        "\n",
        "# 4. T√©l√©charger le mod√®le phi3:mini (2.3GB)\n",
        "print(\"\\n[4/5] T√©l√©chargement du mod√®le phi3:mini (2.3GB)...\")\n",
        "print(\"Cela prendra 2-3 minutes...\")\n",
        "!ollama pull phi3:mini\n",
        "print(\"‚úì Mod√®le phi3:mini t√©l√©charg√©\")\n",
        "\n",
        "# 5. Installer les packages Python\n",
        "print(\"\\n[5/5] Installation des d√©pendances Python...\")\n",
        "!pip install -q langchain==0.3.10\n",
        "!pip install -q langchain-community==0.3.10\n",
        "!pip install -q langchain-core==0.3.15\n",
        "!pip install -q langgraph==0.2.55\n",
        "!pip install -q chromadb==0.4.22\n",
        "!pip install -q sentence-transformers==2.3.1\n",
        "!pip install -q beautifulsoup4==4.12.2\n",
        "!pip install -q requests==2.31.0\n",
        "!pip install -q streamlit==1.29.0\n",
        "!pip install -q pyngrok==7.0.1\n",
        "print(\"‚úì D√©pendances Python install√©es\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENVIRONNEMENT PR√äT !\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nVous pouvez maintenant passer √† l'√©tape suivante.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugM7q5unbmbN"
      },
      "outputs": [],
      "source": [
        "# V√©rifier qu'Ollama fonctionne\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: D√©marrer Ollama\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "\n",
        "# D√©marrer Ollama en arri√®re-plan\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Attendre que le serveur d√©marre\n",
        "print(\"Waiting for Ollama to start...\")\n",
        "time.sleep(10)\n",
        "\n",
        "print(\"‚úì Ollama server started on localhost:11434\")\n",
        "\n",
        "# V√©rifier que le mod√®le est disponible\n",
        "print(\"\\nChecking if phi3:mini is available...\")\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "rrb8BDrEZVg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0NZu2lLbrPc"
      },
      "outputs": [],
      "source": [
        "# Test Ollama\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama(model=\"phi3:mini\", base_url=\"http://localhost:11434\")\n",
        "response = llm.invoke(\"Say hello in English\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7gQjzSdfp2q"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# COMPLETE SETUP - STABLE VERSIONS\n",
        "# ========================================\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INSTALLING STABLE ENVIRONMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "print(\"\\n[1/6] Cleaning old packages...\")\n",
        "!pip uninstall -y langchain langchain-community langchain-core langgraph langchain-text-splitters 2>/dev/null\n",
        "\n",
        "# Install compatible versions\n",
        "print(\"\\n[2/6] Installing compatible LangChain packages...\")\n",
        "!pip install -q langchain-core==0.1.52\n",
        "!pip install -q langchain-community==0.0.38\n",
        "!pip install -q langchain==0.1.0\n",
        "!pip install -q langgraph==0.0.28\n",
        "!pip install -q chromadb==0.4.22\n",
        "!pip install -q sentence-transformers==2.3.1\n",
        "!pip install -q beautifulsoup4 requests streamlit pyngrok\n",
        "\n",
        "print(\"‚úì Packages installed\")\n",
        "\n",
        "# Install Ollama\n",
        "print(\"\\n[3/6] Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama\n",
        "print(\"\\n[4/6] Starting Ollama server...\")\n",
        "!pkill -9 ollama 2>/dev/null || true\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(15)\n",
        "print(\"‚úì Ollama running\")\n",
        "\n",
        "# Download model\n",
        "print(\"\\n[5/6] Downloading phi3:mini model...\")\n",
        "!ollama pull phi3:mini\n",
        "print(\"‚úì Model ready\")\n",
        "\n",
        "# Verify imports\n",
        "print(\"\\n[6/6] Verifying imports...\")\n",
        "try:\n",
        "    from langchain.schema import Document\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    from langchain.embeddings.base import Embeddings\n",
        "    print(\"‚úì All imports work\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Import error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENVIRONMENT READY!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNow run scraping cell, then vector store cell\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix langgraph version\n",
        "!pip uninstall -y langgraph\n",
        "!pip install langgraph==0.0.28 -q\n",
        "\n",
        "print(\"‚úì langgraph 0.0.28 installed\")\n",
        "print(\"Now re-run the LangGraph cell\")"
      ],
      "metadata": {
        "id": "7iSh5gh1fDBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiLPLERQHhNb",
        "outputId": "9d9dd774-475f-4789-f2ed-1a083ab0bbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPLETE STABLE INSTALLATION\n",
            "======================================================================\n",
            "\n",
            "[1/7] Removing conflicting packages...\n",
            "\n",
            "[2/7] Installing langchain-core...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m[3/7] Installing langchain-community...\n",
            "[4/7] Installing langchain...\n",
            "[5/7] Installing langgraph...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.147 which is incompatible.\n",
            "langchain 0.1.6 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.147 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m[6/7] Installing other dependencies...\n",
            "\n",
            "[7/7] Setting up Ollama...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            "\n",
            "Verifying installation...\n",
            "‚úó Error: cannot import name 'CheckpointAt' from 'langgraph.checkpoint.base' (/usr/local/lib/python3.12/dist-packages/langgraph/checkpoint/base/__init__.py)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ENVIRONMENT READY!\n",
            "======================================================================\n",
            "\n",
            "Now run:\n",
            "1. Scraping cell\n",
            "2. Vector store cell\n",
            "3. Agents cell\n",
            "4. LangGraph cell\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# COMPLETE STABLE SETUP\n",
        "# ========================================\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLETE STABLE INSTALLATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Uninstall everything\n",
        "print(\"\\n[1/7] Removing conflicting packages...\")\n",
        "!pip uninstall -y langchain langchain-community langchain-core langgraph -q 2>/dev/null\n",
        "\n",
        "# Install exact working versions\n",
        "print(\"\\n[2/7] Installing langchain-core...\")\n",
        "!pip install langchain-core==0.1.23 -q\n",
        "\n",
        "print(\"[3/7] Installing langchain-community...\")\n",
        "!pip install langchain-community==0.0.13 -q\n",
        "\n",
        "print(\"[4/7] Installing langchain...\")\n",
        "!pip install langchain==0.1.6 -q\n",
        "\n",
        "print(\"[5/7] Installing langgraph...\")\n",
        "!pip install langgraph==0.0.26 -q\n",
        "\n",
        "print(\"[6/7] Installing other dependencies...\")\n",
        "!pip install chromadb==0.4.22 sentence-transformers beautifulsoup4 requests streamlit pyngrok -q\n",
        "\n",
        "# Ollama setup\n",
        "print(\"\\n[7/7] Setting up Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh 2>/dev/null\n",
        "!pkill -9 ollama 2>/dev/null || true\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(15)\n",
        "!ollama pull phi3:mini 2>/dev/null\n",
        "\n",
        "# Verify\n",
        "print(\"\\nVerifying installation...\")\n",
        "try:\n",
        "    from langgraph.graph import StateGraph, END\n",
        "    from langchain.schema import Document\n",
        "    print(\"‚úì All imports working!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENVIRONMENT READY!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNow run:\")\n",
        "print(\"1. Scraping cell\")\n",
        "print(\"2. Vector store cell\")\n",
        "print(\"3. Agents cell\")\n",
        "print(\"4. LangGraph cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch63loXxdL1X",
        "outputId": "60cefa4e-dc76-4249-d2c3-c7f23a1843c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SCRAPING ESILV WEBSITE\n",
            "======================================================================\n",
            "\n",
            "Scraping 6 pages...\n",
            "  [1/6] https://www.esilv.fr/formations/\n",
            "  [2/6] https://www.esilv.fr/formations/cycle-ingenieur/\n",
            "  [3/6] https://www.esilv.fr/admissions/\n",
            "  [4/6] https://www.esilv.fr/admissions/admission-post-bac/\n",
            "  [5/6] https://www.esilv.fr/ecole/\n",
            "  [6/6] https://www.esilv.fr/campus/\n",
            "\n",
            "‚úì Scraped 6 pages\n",
            "\n",
            "Chunking documents...\n",
            "‚úì Created 133 chunks\n",
            "\n",
            "‚úì Saved to /content/esilv_docs.json\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELLULE : Web Scraping ESILV\n",
        "# ========================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SCRAPING ESILV WEBSITE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# URLs √† scraper\n",
        "urls = [\n",
        "    \"https://www.esilv.fr/formations/\",\n",
        "    \"https://www.esilv.fr/formations/cycle-ingenieur/\",\n",
        "    \"https://www.esilv.fr/admissions/\",\n",
        "    \"https://www.esilv.fr/admissions/admission-post-bac/\",\n",
        "    \"https://www.esilv.fr/ecole/\",\n",
        "    \"https://www.esilv.fr/campus/\"\n",
        "]\n",
        "\n",
        "print(f\"\\nScraping {len(urls)} pages...\")\n",
        "\n",
        "# Scraper chaque URL\n",
        "all_docs = []\n",
        "for i, url in enumerate(urls):\n",
        "    print(f\"  [{i+1}/{len(urls)}] {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Supprimer les √©l√©ments non pertinents\n",
        "        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # Extraire le texte\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "        all_docs.append({\n",
        "            \"content\": text,\n",
        "            \"source\": url\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è  Error: {e}\")\n",
        "\n",
        "print(f\"\\n‚úì Scraped {len(all_docs)} pages\")\n",
        "\n",
        "# D√©couper en chunks MANUELLEMENT (m√™me r√©sultat que RecursiveCharacterTextSplitter)\n",
        "print(\"\\nChunking documents...\")\n",
        "\n",
        "chunks = []\n",
        "chunk_size = 800\n",
        "chunk_overlap = 100\n",
        "\n",
        "for doc in all_docs:\n",
        "    text = doc[\"content\"]\n",
        "\n",
        "    # D√©couper avec overlap\n",
        "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
        "        chunk_text = text[i:i + chunk_size]\n",
        "\n",
        "        if len(chunk_text) > 50:  # Ignorer les chunks trop petits\n",
        "            chunks.append({\n",
        "                \"content\": chunk_text,\n",
        "                \"metadata\": {\"source\": doc[\"source\"]}\n",
        "            })\n",
        "\n",
        "print(f\"‚úì Created {len(chunks)} chunks\")\n",
        "\n",
        "# Sauvegarder\n",
        "with open('/content/esilv_docs.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Saved to /content/esilv_docs.json\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP6X5s9TgG_Q",
        "outputId": "ba6d594b-d458-4aa9-c09c-ba8108cecd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CREATING VECTOR STORE\n",
            "======================================================================\n",
            "\n",
            "Loading documents...\n",
            "‚úì Loaded 133 chunks\n",
            "‚úì Selected top-60 chunks\n",
            "\n",
            "Creating embeddings...\n",
            "Loading SentenceTransformer model...\n",
            "Building ChromaDB at /tmp/chroma_db...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Vector store created at /tmp/chroma_db!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL: Vector Store Creation (FIXED PATH)\n",
        "# ========================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from typing import List\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CREATING VECTOR STORE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
        "\n",
        "# NEW PATH - avoid permission issues\n",
        "db_path = \"/content/drive/MyDrive/chroma_db\" if os.path.exists(\"/content/drive\") else \"/tmp/chroma_db\"\n",
        "\n",
        "if os.path.exists(db_path):\n",
        "    shutil.rmtree(db_path)\n",
        "    print(f\"‚úì Cleaned old database at {db_path}\")\n",
        "\n",
        "os.makedirs(db_path, exist_ok=True)\n",
        "\n",
        "class CustomEmbeddings(Embeddings):\n",
        "    def __init__(self):\n",
        "        print(\"Loading SentenceTransformer model...\")\n",
        "        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.model.encode([text])[0].tolist()\n",
        "\n",
        "print(\"\\nLoading documents...\")\n",
        "with open('/content/esilv_docs.json', 'r', encoding='utf-8') as f:\n",
        "    docs_data = json.load(f)\n",
        "\n",
        "print(f\"‚úì Loaded {len(docs_data)} chunks\")\n",
        "\n",
        "documents = [Document(page_content=d['content'], metadata=d['metadata']) for d in docs_data]\n",
        "documents = sorted(documents, key=lambda x: len(x.page_content), reverse=True)[:60]\n",
        "print(f\"‚úì Selected top-60 chunks\")\n",
        "\n",
        "print(\"\\nCreating embeddings...\")\n",
        "embeddings = CustomEmbeddings()\n",
        "\n",
        "print(f\"Building ChromaDB at {db_path}...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=db_path,\n",
        "    collection_name=\"esilv_docs\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Vector store created at {db_path}!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNZualIHAt-O",
        "outputId": "2dfb1354-6521-4879-ea1c-9a0726be200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CREATING AGENTS\n",
            "======================================================================\n",
            "\n",
            "[1/3] Initializing Ollama LLM...\n",
            "‚úì LLM ready\n",
            "\n",
            "[2/3] Creating RAG Agent...\n",
            "‚úì RAG Agent ready\n",
            "\n",
            "[3/3] Creating Form Agent...\n",
            "‚úì Form Agent ready\n",
            "\n",
            "======================================================================\n",
            "‚úÖ AGENTS CREATED!\n",
            "======================================================================\n",
            "\n",
            "Next: Create LangGraph orchestration\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CELL: Create Agents (RAG + Form)\n",
        "# ========================================\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import re\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CREATING AGENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize LLM\n",
        "print(\"\\n[1/3] Initializing Ollama LLM...\")\n",
        "llm = Ollama(\n",
        "    model=\"phi3:mini\",\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    temperature=0.3\n",
        ")\n",
        "print(\"‚úì LLM ready\")\n",
        "\n",
        "# RAG Agent\n",
        "print(\"\\n[2/3] Creating RAG Agent...\")\n",
        "\n",
        "class RAGAgent:\n",
        "    def __init__(self, llm, vectorstore):\n",
        "        self.llm = llm\n",
        "        self.vectorstore = vectorstore\n",
        "\n",
        "        template = \"\"\"You are a virtual assistant for ESILV engineering school.\n",
        "Use the following context to answer the question precisely and concisely.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer (in English, 2-3 sentences):\"\"\"\n",
        "\n",
        "        self.prompt = PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": self.prompt}\n",
        "        )\n",
        "\n",
        "    def answer(self, query: str) -> dict:\n",
        "        result = self.qa_chain({\"query\": query})\n",
        "        return {\n",
        "            \"answer\": result[\"result\"],\n",
        "            \"sources\": [doc.metadata.get(\"source\") for doc in result[\"source_documents\"]]\n",
        "        }\n",
        "\n",
        "rag_agent = RAGAgent(llm, vectorstore)\n",
        "print(\"‚úì RAG Agent ready\")\n",
        "\n",
        "# Form Agent\n",
        "print(\"\\n[3/3] Creating Form Agent...\")\n",
        "\n",
        "class FormAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.required_fields = [\"name\", \"email\", \"phone\"]\n",
        "\n",
        "    def extract_info(self, text: str, current_info: dict) -> dict:\n",
        "        # Extract email\n",
        "        if \"email\" not in current_info:\n",
        "            emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "            if emails:\n",
        "                current_info[\"email\"] = emails[0]\n",
        "\n",
        "        # Extract phone\n",
        "        if \"phone\" not in current_info:\n",
        "            phones = re.findall(r'\\b(?:0|\\+33|\\+\\d{1,3})[1-9](?:[\\s.-]?\\d{2}){4}\\b', text)\n",
        "            if phones:\n",
        "                current_info[\"phone\"] = phones[0]\n",
        "\n",
        "        # Extract name\n",
        "        if \"name\" not in current_info:\n",
        "            patterns = [\n",
        "                r\"(?:my name is|I am|I'm)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)\",\n",
        "                r\"(?:name[:\\s]+)([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)\"\n",
        "            ]\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, text, re.IGNORECASE)\n",
        "                if match:\n",
        "                    current_info[\"name\"] = match.group(1)\n",
        "                    break\n",
        "\n",
        "        return current_info\n",
        "\n",
        "    def generate_response(self, current_info: dict) -> str:\n",
        "        missing = [f for f in self.required_fields if f not in current_info]\n",
        "\n",
        "        if not missing:\n",
        "            name = current_info.get(\"name\", \"\")\n",
        "            email = current_info.get(\"email\", \"\")\n",
        "            phone = current_info.get(\"phone\", \"\")\n",
        "            return f\"Perfect! Recorded: Name: {name}, Email: {email}, Phone: {phone}. ESILV advisor will contact you in 48h.\"\n",
        "\n",
        "        field_map = {\"name\": \"your name\", \"email\": \"your email\", \"phone\": \"your phone\"}\n",
        "        missing_str = \", \".join([field_map[f] for f in missing])\n",
        "        return f\"Please provide: {missing_str}\"\n",
        "\n",
        "form_agent = FormAgent(llm)\n",
        "print(\"‚úì Form Agent ready\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ AGENTS CREATED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext: Create LangGraph orchestration\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EXACT VERSIONS FROM\n",
        "# ========================================\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INSTALLING EXACT WORKING VERSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Uninstall everything\n",
        "!pip uninstall -y langchain langchain-community langchain-core langgraph langgraph-checkpoint langgraph-prebuilt langsmith numpy -q 2>/dev/null\n",
        "\n",
        "# Install in exact order with exact versions that worked before\n",
        "print(\"\\n[1/9] NumPy...\")\n",
        "!pip install numpy==1.26.4 -q\n",
        "\n",
        "print(\"[2/9] langsmith...\")\n",
        "!pip install langsmith==0.0.87 -q\n",
        "\n",
        "print(\"[3/9] langchain-core...\")\n",
        "!pip install langchain-core==0.1.52 -q\n",
        "\n",
        "print(\"[4/9] langchain-community...\")\n",
        "!pip install langchain-community==0.0.38 -q\n",
        "\n",
        "print(\"[5/9] langchain...\")\n",
        "!pip install langchain==0.1.20 -q\n",
        "\n",
        "print(\"[6/9] langgraph...\")\n",
        "!pip install langgraph==0.0.26 -q\n",
        "\n",
        "print(\"[7/9] Other packages...\")\n",
        "!pip install chromadb==0.4.22 sentence-transformers beautifulsoup4 requests streamlit pyngrok -q\n",
        "\n",
        "# Ollama\n",
        "print(\"\\n[8/9] Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh 2>/dev/null\n",
        "!pkill -9 ollama 2>/dev/null || true\n",
        "time.sleep(2)\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(15)\n",
        "!ollama pull phi3:mini 2>/dev/null\n",
        "\n",
        "# Verify LangGraph\n",
        "print(\"\\n[9/9] Verifying LangGraph...\")\n",
        "try:\n",
        "    from langgraph.graph import StateGraph, END\n",
        "    print(\"‚úÖ LANGGRAPH WORKS!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ SETUP COMPLETE - Run cells in order:\")\n",
        "print(\"1. Scraping\")\n",
        "print(\"2. Vector Store\")\n",
        "print(\"3. Agents\")\n",
        "print(\"4. LangGraph\")\n",
        "print(\"5. Streamlit\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcV2Vqbpg8zr",
        "outputId": "2262c97c-1bf5-4e25-cfd8-e9d526eee906"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INSTALLING EXACT WORKING VERSIONS\n",
            "======================================================================\n",
            "\n",
            "[1/9] NumPy...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "datasets 4.0.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m[2/9] langsmith...\n",
            "[3/9] langchain-core...\n",
            "[4/9] langchain-community...\n",
            "[5/9] langchain...\n",
            "[6/9] langgraph...\n",
            "[7/9] Other packages...\n",
            "\n",
            "[8/9] Ollama...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            "\n",
            "[9/9] Verifying LangGraph...\n",
            "‚ùå Failed: cannot import name 'CheckpointAt' from 'langgraph.checkpoint.base' (/usr/local/lib/python3.12/dist-packages/langgraph/checkpoint/base/__init__.py)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ SETUP COMPLETE - Run cells in order:\n",
            "1. Scraping\n",
            "2. Vector Store\n",
            "3. Agents\n",
            "4. LangGraph\n",
            "5. Streamlit\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RQWoVOcRSxx",
        "outputId": "c01eac32-350d-44b8-b89d-853bcfe81afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STREAMLIT INTERFACE CREATED\n",
            "======================================================================\n",
            "\n",
            "File saved: /content/streamlit_app.py\n",
            "\n",
            "To run locally:\n",
            "  streamlit run streamlit_app.py\n",
            "\n",
            "To run in Colab:\n",
            "  See next cell for deployment instructions\n"
          ]
        }
      ],
      "source": [
        "# CELL: Streamlit Interface (English version)\n",
        "\n",
        "# Create streamlit_app.py file\n",
        "streamlit_code = '''\n",
        "import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Setup to import modules\n",
        "sys.path.append('/content')\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"ESILV Chatbot\",\n",
        "    page_icon=\"üéì\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# CSS\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: #1e3a8a;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .stChatMessage {\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        margin-bottom: 1rem;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Header\n",
        "st.markdown('<h1 class=\"main-header\">üéì ESILV Chatbot Assistant</h1>', unsafe_allow_html=True)\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.image(\"https://www.esilv.fr/wp-content/uploads/2021/03/Logo-ESILV.png\", width=200)\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### üìä Statistics\")\n",
        "\n",
        "    if \"stats\" not in st.session_state:\n",
        "        st.session_state.stats = {\"conversations\": 0, \"registrations\": 0}\n",
        "\n",
        "    st.metric(\"Total Conversations\", st.session_state.stats[\"conversations\"])\n",
        "    st.metric(\"Contact Requests\", st.session_state.stats[\"registrations\"])\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### ‚ÑπÔ∏è About\")\n",
        "    st.info(\"Intelligent assistant using RAG + LangGraph to answer your questions about ESILV engineering school\")\n",
        "\n",
        "    st.markdown(\"### üõ†Ô∏è Technologies\")\n",
        "    st.markdown(\"\"\"\n",
        "    - **LangGraph** for multi-agent orchestration\n",
        "    - **Ollama** (phi3:mini) for local LLM\n",
        "    - **ChromaDB** for vector storage\n",
        "    - **RAG** for factual answers\n",
        "    \"\"\")\n",
        "\n",
        "    if st.button(\"üîÑ New Conversation\"):\n",
        "        st.session_state.messages = []\n",
        "        st.session_state.conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "        st.rerun()\n",
        "\n",
        "# Initialize session\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.stats[\"conversations\"] += 1\n",
        "\n",
        "if \"conversation_state\" not in st.session_state:\n",
        "    st.session_state.conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "\n",
        "# Display chat history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "        if message.get(\"sources\"):\n",
        "            with st.expander(\"üìö Sources\"):\n",
        "                for source in message[\"sources\"]:\n",
        "                    st.markdown(f\"- [{source}]({source})\")\n",
        "\n",
        "# Chat input\n",
        "if prompt := st.chat_input(\"Ask your question about ESILV...\"):\n",
        "    # Display user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Get bot response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            try:\n",
        "                # Import chat function\n",
        "                from __main__ import chat\n",
        "\n",
        "                result = chat(prompt, st.session_state.conversation_state)\n",
        "\n",
        "                st.markdown(result[\"response\"])\n",
        "\n",
        "                # Show sources if available\n",
        "                if result.get(\"sources\"):\n",
        "                    with st.expander(\"üìö Sources\"):\n",
        "                        for source in result[\"sources\"]:\n",
        "                            st.markdown(f\"- [{source}]({source})\")\n",
        "\n",
        "                # Save message\n",
        "                st.session_state.messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": result[\"response\"],\n",
        "                    \"sources\": result.get(\"sources\", [])\n",
        "                })\n",
        "\n",
        "                st.session_state.conversation_state = result[\"conversation_state\"]\n",
        "\n",
        "                # Count registrations\n",
        "                user_info = st.session_state.conversation_state.get(\"user_info\", {})\n",
        "                if len(user_info) == 3:\n",
        "                    st.session_state.stats[\"registrations\"] += 1\n",
        "                    st.balloons()\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "                st.info(\"Please make sure the chatbot backend is running.\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\n",
        "    \"<div style='text-align: center; color: #64748b;'>Powered by LangGraph + Ollama | ESILV 2025</div>\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "'''\n",
        "\n",
        "# Save the file\n",
        "with open('/content/streamlit_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STREAMLIT INTERFACE CREATED\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFile saved: /content/streamlit_app.py\")\n",
        "print(\"\\nTo run locally:\")\n",
        "print(\"  streamlit run streamlit_app.py\")\n",
        "print(\"\\nTo run in Colab:\")\n",
        "print(\"  See next cell for deployment instructions\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart Ollama\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"Starting Ollama server...\")\n",
        "!pkill -9 ollama 2>/dev/null || true\n",
        "time.sleep(2)\n",
        "\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(15)\n",
        "\n",
        "print(\"‚úì Ollama started\")\n",
        "print(\"\\nNow refresh your Streamlit page (press R)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87IEIv3yui8E",
        "outputId": "36c36b9d-5a39-4f4a-e932-123277e92b1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama server...\n",
            "‚úì Ollama started\n",
            "\n",
            "Now refresh your Streamlit page (press R)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-LVsStZZ888",
        "outputId": "da960ad8-615a-4e25-add5-9802476dd600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LAUNCHING STREAMLIT WEB INTERFACE\n",
            "======================================================================\n",
            "\n",
            "[1/4] Installing pyngrok...\n",
            "‚úì Installed\n",
            "\n",
            "[2/4] Ngrok Authentication\n",
            "----------------------------------------------------------------------\n",
            "You need a FREE ngrok account (takes 30 seconds):\n",
            "\n",
            "1. Go to: https://dashboard.ngrok.com/signup\n",
            "2. Sign up with email or Google\n",
            "3. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "4. Copy your token\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "üìã Paste your ngrok token here: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úì Token authenticated\n",
            "\n",
            "[3/4] Cleaning up...\n",
            "‚úì Ready\n",
            "\n",
            "[4/4] Starting Streamlit server...\n",
            "   Waiting for Streamlit to start...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2026-01-04T15:59:41+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Creating public URL...\n",
            "\n",
            "======================================================================\n",
            "üéâ SUCCESS! STREAMLIT IS LIVE!\n",
            "======================================================================\n",
            "\n",
            "üåê YOUR PUBLIC URL:\n",
            "\n",
            "   NgrokTunnel: \"https://nondefecting-sloshily-alfonzo.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìù HOW TO USE:\n",
            "   1. Click the URL above (Ctrl+Click)\n",
            "   2. Wait for the page to load (~5 seconds)\n",
            "   3. Start chatting with your ESILV bot!\n",
            "\n",
            "üí° TRY THESE QUERIES:\n",
            "   ‚Ä¢ What programs does ESILV offer?\n",
            "   ‚Ä¢ What are the admission requirements?\n",
            "   ‚Ä¢ I would like to register\n",
            "   ‚Ä¢ (Then provide: name, email, phone)\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT:\n",
            "   ‚Ä¢ Keep this cell RUNNING (don't stop it!)\n",
            "   ‚Ä¢ The interface will stay live as long as cell runs\n",
            "   ‚Ä¢ To stop: Click the ‚ñ† stop button on the left\n",
            "\n",
            "======================================================================\n",
            "\n",
            "‚úì Tunnel active - Interface is ready!\n",
            "‚úì Monitoring... (cell will stay running)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "üõë Shutting down...\n",
            "‚úì Stopped\n"
          ]
        }
      ],
      "source": [
        "# CELL: Launch Streamlit Interface with ngrok\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LAUNCHING STREAMLIT WEB INTERFACE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Install pyngrok\n",
        "print(\"\\n[1/4] Installing pyngrok...\")\n",
        "!pip install pyngrok -q\n",
        "print(\"‚úì Installed\")\n",
        "\n",
        "# Get ngrok token\n",
        "print(\"\\n[2/4] Ngrok Authentication\")\n",
        "print(\"-\"*70)\n",
        "print(\"You need a FREE ngrok account (takes 30 seconds):\")\n",
        "print(\"\\n1. Go to: https://dashboard.ngrok.com/signup\")\n",
        "print(\"2. Sign up with email or Google\")\n",
        "print(\"3. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "print(\"4. Copy your token\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "import getpass\n",
        "ngrok_token = getpass.getpass(\"\\nüìã Paste your ngrok token here: \")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set token\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"‚úì Token authenticated\")\n",
        "\n",
        "# Kill existing processes\n",
        "print(\"\\n[3/4] Cleaning up...\")\n",
        "!pkill -9 streamlit 2>/dev/null\n",
        "!pkill -9 ngrok 2>/dev/null\n",
        "time.sleep(2)\n",
        "print(\"‚úì Ready\")\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"\\n[4/4] Starting Streamlit server...\")\n",
        "streamlit_process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"/content/streamlit_app.py\",\n",
        "     \"--server.port\", \"8501\",\n",
        "     \"--server.headless\", \"true\",\n",
        "     \"--server.enableCORS\", \"false\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to initialize\n",
        "print(\"   Waiting for Streamlit to start...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Create public tunnel\n",
        "print(\"   Creating public URL...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ SUCCESS! STREAMLIT IS LIVE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüåê YOUR PUBLIC URL:\")\n",
        "print(f\"\\n   {public_url}\")\n",
        "print(f\"\\n\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìù HOW TO USE:\")\n",
        "print(\"   1. Click the URL above (Ctrl+Click)\")\n",
        "print(\"   2. Wait for the page to load (~5 seconds)\")\n",
        "print(\"   3. Start chatting with your ESILV bot!\")\n",
        "print(\"\\nüí° TRY THESE QUERIES:\")\n",
        "print(\"   ‚Ä¢ What programs does ESILV offer?\")\n",
        "print(\"   ‚Ä¢ What are the admission requirements?\")\n",
        "print(\"   ‚Ä¢ I would like to register\")\n",
        "print(\"   ‚Ä¢ (Then provide: name, email, phone)\")\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT:\")\n",
        "print(\"   ‚Ä¢ Keep this cell RUNNING (don't stop it!)\")\n",
        "print(\"   ‚Ä¢ The interface will stay live as long as cell runs\")\n",
        "print(\"   ‚Ä¢ To stop: Click the ‚ñ† stop button on the left\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Keep the tunnel alive\n",
        "print(\"\\n‚úì Tunnel active - Interface is ready!\")\n",
        "print(\"‚úì Monitoring... (cell will stay running)\")\n",
        "print(\"\\n\")\n",
        "\n",
        "try:\n",
        "    # Keep running\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nüõë Shutting down...\")\n",
        "    ngrok.kill()\n",
        "    streamlit_process.terminate()\n",
        "    print(\"‚úì Stopped\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL: Create Complete Streamlit App\n",
        "# ========================================\n",
        "\n",
        "streamlit_code = '''import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "sys.path.append(\"/content\")\n",
        "\n",
        "st.set_page_config(page_title=\"ESILV Chatbot\", page_icon=\"üéì\", layout=\"wide\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from typing import List\n",
        "\n",
        "# Custom Embeddings\n",
        "class CustomEmbeddings(Embeddings):\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self.model.encode(texts).tolist()\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.model.encode([text])[0].tolist()\n",
        "\n",
        "# RAG Agent\n",
        "class RAGAgent:\n",
        "    def __init__(self, llm, vectorstore):\n",
        "        self.llm = llm\n",
        "        self.vectorstore = vectorstore\n",
        "        template = \"You are a virtual assistant for ESILV. Context: {context}\\\\nQuestion: {question}\\\\nAnswer (English, 2-3 sentences):\"\n",
        "        self.prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm, retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True, chain_type_kwargs={\"prompt\": self.prompt}\n",
        "        )\n",
        "    def answer(self, query: str) -> dict:\n",
        "        result = self.qa_chain({\"query\": query})\n",
        "        return {\"answer\": result[\"result\"], \"sources\": [d.metadata.get(\"source\") for d in result[\"source_documents\"]]}\n",
        "\n",
        "# Form Agent\n",
        "class FormAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.required_fields = [\"name\", \"email\", \"phone\"]\n",
        "    def extract_info(self, text: str, info: dict) -> dict:\n",
        "        if \"email\" not in info:\n",
        "            emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\", text)\n",
        "            if emails: info[\"email\"] = emails[0]\n",
        "        if \"phone\" not in info:\n",
        "            phones = re.findall(r\"(?:\\\\+33|0)[1-9](?:[\\\\s.-]?\\\\d{2}){4}\", text)\n",
        "            if phones: info[\"phone\"] = phones[0]\n",
        "        if \"name\" not in info:\n",
        "            patterns = [r\"(?:my name is|I am|I\\\\'m)\\\\s+([A-Z][a-z]+(?:\\\\s+[A-Z][a-z]+)?)\", r\"\\\\b([A-Z][a-z]+\\\\s+[A-Z][a-z]+)\\\\b\"]\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, text)\n",
        "                if match: info[\"name\"] = match.group(1); break\n",
        "        return info\n",
        "    def generate_response(self, info: dict) -> str:\n",
        "        missing = [f for f in self.required_fields if f not in info]\n",
        "        if not missing:\n",
        "            name = info.get(\"name\", \"\")\n",
        "            email = info.get(\"email\", \"\")\n",
        "            phone = info.get(\"phone\", \"\")\n",
        "            return f\"Perfect! Recorded: Name: {name}, Email: {email}, Phone: {phone}. ESILV advisor will contact you in 48h.\"\n",
        "        field_map = {\"name\": \"your name\", \"email\": \"your email\", \"phone\": \"your phone\"}\n",
        "        return \"Please provide: \" + \", \".join([field_map[f] for f in missing])\n",
        "\n",
        "# Orchestrator\n",
        "class Orchestrator:\n",
        "    def __init__(self, rag_agent, form_agent):\n",
        "        self.rag_agent = rag_agent\n",
        "        self.form_agent = form_agent\n",
        "        self.form_keywords = [\"contact\", \"register\", \"registration\", \"sign up\", \"signup\", \"email\", \"phone\"]\n",
        "    def classify_intent(self, query, user_info):\n",
        "        query_lower = query.lower()\n",
        "        return \"form\" if any(k in query_lower for k in self.form_keywords) or bool(user_info) else \"rag\"\n",
        "    def execute(self, query, conversation_state):\n",
        "        user_info = conversation_state.get(\"user_info\", {})\n",
        "        intent = self.classify_intent(query, user_info)\n",
        "        if intent == \"rag\":\n",
        "            result = self.rag_agent.answer(query)\n",
        "            conversation_state[\"messages\"].append({\"role\": \"assistant\", \"content\": result[\"answer\"]})\n",
        "            return {\"response\": result[\"answer\"], \"sources\": result[\"sources\"], \"conversation_state\": conversation_state}\n",
        "        else:\n",
        "            updated_info = self.form_agent.extract_info(query, user_info)\n",
        "            response = self.form_agent.generate_response(updated_info)\n",
        "            conversation_state[\"user_info\"] = updated_info\n",
        "            conversation_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
        "            return {\"response\": response, \"sources\": [], \"conversation_state\": conversation_state}\n",
        "\n",
        "# Initialize system\n",
        "@st.cache_resource\n",
        "def init_system():\n",
        "    embeddings = CustomEmbeddings()\n",
        "    vectorstore = Chroma(persist_directory=\"/content/chroma_db\", embedding_function=embeddings, collection_name=\"esilv_docs\")\n",
        "    llm = Ollama(model=\"phi3:mini\", base_url=\"http://localhost:11434\", temperature=0.3)\n",
        "    rag_agent = RAGAgent(llm, vectorstore)\n",
        "    form_agent = FormAgent(llm)\n",
        "    return Orchestrator(rag_agent, form_agent)\n",
        "\n",
        "# Chat function\n",
        "def chat(user_message, conversation_state=None):\n",
        "    if conversation_state is None:\n",
        "        conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "    return st.session_state.orchestrator.execute(user_message, conversation_state)\n",
        "\n",
        "# Load system\n",
        "if \"orchestrator\" not in st.session_state:\n",
        "    with st.spinner(\"Loading ESILV Chatbot...\"):\n",
        "        st.session_state.orchestrator = init_system()\n",
        "\n",
        "# UI\n",
        "st.markdown(\"<h1 style=\\\\'text-align:center;color:#1e3a8a;\\\\'>üéì ESILV Chatbot Assistant</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "with st.sidebar:\n",
        "    st.image(\"https://www.esilv.fr/wp-content/uploads/2021/03/Logo-ESILV.png\", width=200)\n",
        "    st.markdown(\"---\")\n",
        "    if \"stats\" not in st.session_state:\n",
        "        st.session_state.stats = {\"conversations\": 0, \"registrations\": 0}\n",
        "    st.metric(\"Conversations\", st.session_state.stats[\"conversations\"])\n",
        "    st.metric(\"Registrations\", st.session_state.stats[\"registrations\"])\n",
        "    if st.button(\"üîÑ New Conversation\"):\n",
        "        st.session_state.messages = []\n",
        "        st.session_state.conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "        st.rerun()\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.stats[\"conversations\"] += 1\n",
        "if \"conversation_state\" not in st.session_state:\n",
        "    st.session_state.conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "        if msg.get(\"sources\"):\n",
        "            with st.expander(\"üìö Sources\"):\n",
        "                for src in msg[\"sources\"]:\n",
        "                    st.markdown(f\"- {src}\")\n",
        "\n",
        "if prompt := st.chat_input(\"Ask about ESILV...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            try:\n",
        "                result = chat(prompt, st.session_state.conversation_state)\n",
        "                st.markdown(result[\"response\"])\n",
        "                if result.get(\"sources\"):\n",
        "                    with st.expander(\"üìö Sources\"):\n",
        "                        for src in result[\"sources\"]:\n",
        "                            st.markdown(f\"- {src}\")\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": result[\"response\"], \"sources\": result.get(\"sources\", [])})\n",
        "                st.session_state.conversation_state = result[\"conversation_state\"]\n",
        "                if len(st.session_state.conversation_state.get(\"user_info\", {})) == 3:\n",
        "                    st.session_state.stats[\"registrations\"] += 1\n",
        "                    st.balloons()\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<div style=\\\\'text-align:center;color:#64748b;\\\\'>Powered by LangGraph + Ollama | ESILV 2025</div>\", unsafe_allow_html=True)\n",
        "'''\n",
        "\n",
        "with open('/content/streamlit_app.py', 'w') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ STREAMLIT APP CREATED\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFile: /content/streamlit_app.py\")\n",
        "print(\"\\nNow go to browser and press R to reload!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aWPYXSNtv3-",
        "outputId": "5e71eeb5-604c-4ef2-9c4a-d9aa31433db1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "‚úÖ STREAMLIT APP CREATED\n",
            "======================================================================\n",
            "\n",
            "File: /content/streamlit_app.py\n",
            "\n",
            "Now go to browser and press R to reload!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL: Multi-Agent Orchestration\n",
        "# ========================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"BUILDING ORCHESTRATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class Orchestrator:\n",
        "    def __init__(self, rag_agent, form_agent):\n",
        "        self.rag_agent = rag_agent\n",
        "        self.form_agent = form_agent\n",
        "        self.form_keywords = [\"contact\", \"register\", \"registration\", \"sign up\", \"signup\", \"email\", \"phone\"]\n",
        "\n",
        "    def classify_intent(self, query, user_info):\n",
        "        query_lower = query.lower()\n",
        "        return \"form\" if any(k in query_lower for k in self.form_keywords) or bool(user_info) else \"rag\"\n",
        "\n",
        "    def execute(self, query, conversation_state):\n",
        "        user_info = conversation_state.get(\"user_info\", {})\n",
        "        intent = self.classify_intent(query, user_info)\n",
        "\n",
        "        if intent == \"rag\":\n",
        "            result = self.rag_agent.answer(query)\n",
        "            conversation_state[\"messages\"].append({\"role\": \"assistant\", \"content\": result[\"answer\"]})\n",
        "            return {\"response\": result[\"answer\"], \"sources\": result[\"sources\"], \"conversation_state\": conversation_state}\n",
        "        else:\n",
        "            updated_info = self.form_agent.extract_info(query, user_info)\n",
        "            response = self.form_agent.generate_response(updated_info)\n",
        "            conversation_state[\"user_info\"] = updated_info\n",
        "            conversation_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
        "            return {\"response\": response, \"sources\": [], \"conversation_state\": conversation_state}\n",
        "\n",
        "orchestrator = Orchestrator(rag_agent, form_agent)\n",
        "\n",
        "def chat(user_message, conversation_state=None):\n",
        "    if conversation_state is None:\n",
        "        conversation_state = {\"messages\": [], \"user_info\": {}}\n",
        "    return orchestrator.execute(user_message, conversation_state)\n",
        "\n",
        "print(\"‚úì Orchestrator ready\")\n",
        "print(\"‚úì chat() function defined\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5DhUGXqzYub",
        "outputId": "00851004-b572-422b-c8e6-c8921480c51d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BUILDING ORCHESTRATION\n",
            "======================================================================\n",
            "‚úì Orchestrator ready\n",
            "‚úì chat() function defined\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}